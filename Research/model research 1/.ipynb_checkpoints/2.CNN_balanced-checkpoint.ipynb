{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\GAN for Face expression Classification\\\\Research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\GAN for Face expression Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "logging.basicConfig(\n",
    "    # filename='extract_data.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras. layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_size = 48\n",
    "folder_path = Path(os.getcwd()) / \"Dataset/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 128\n",
    "datagen_train  = ImageDataGenerator()\n",
    "datagen_val = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "class CopyAndAugmentBalancedDataset:\n",
    "    def __init__(self, source_directory, copy_directory, target_size=(256, 256)):\n",
    "        self.source_directory = Path(source_directory)\n",
    "        self.copy_directory = Path(copy_directory)\n",
    "        self.target_size = target_size\n",
    "        self.class_indices = self._get_class_indices()\n",
    "        # Calculate min_count after obtaining all class indices to ensure it is available when needed\n",
    "        self.min_count = min(len(files) for files in self.class_indices.values())\n",
    "        self._log_class_info()\n",
    "\n",
    "    def _get_class_indices(self):\n",
    "        \"\"\"Scan the source directory and map each class to its images.\"\"\"\n",
    "        class_indices = {}\n",
    "        for class_name in os.listdir(self.source_directory):\n",
    "            class_path = self.source_directory / class_name\n",
    "            if class_path.is_dir():\n",
    "                class_indices[class_name] = [str(class_path / fname) for fname in os.listdir(class_path)]\n",
    "        return class_indices\n",
    "\n",
    "    def _log_class_info(self):\n",
    "        \"\"\"Log information about the classes and their image counts.\"\"\"\n",
    "        logging.info(f\"Number of unique classes: {len(self.class_indices)}\")\n",
    "        for class_name, files in self.class_indices.items():\n",
    "            logging.info(f\"Count of images in {class_name}: {len(files)}\")\n",
    "        logging.info(f\"Minimum frequency among classes: {self.min_count}\")\n",
    "\n",
    "    def _augment_and_copy_image(self, source_path, target_path):\n",
    "        \"\"\"Resize image and save to target path.\"\"\"\n",
    "        try:\n",
    "            with Image.open(source_path) as img:\n",
    "                img_resized = img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "                img_resized.save(target_path)\n",
    "            logging.debug(f\"Image {source_path} resized and copied to {target_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing image {source_path}: {e}\")\n",
    "\n",
    "    def copy_and_augment_balanced_dataset(self):\n",
    "        \"\"\"Copy and augment a balanced dataset to the copy directory.\"\"\"\n",
    "        logging.info(\"Starting to copy and augment the dataset...\")\n",
    "        for class_name, files in self.class_indices.items():\n",
    "            copy_path = self.copy_directory / class_name\n",
    "            copy_path.mkdir(parents=True, exist_ok=True)\n",
    "            # Ensure we only process up to min_count images per class\n",
    "            selected_files = files[:self.min_count]\n",
    "            for file_path in selected_files:\n",
    "                target_file_path = copy_path / Path(file_path).name\n",
    "                self._augment_and_copy_image(file_path, target_file_path)\n",
    "        logging.info(f\"Augmented balanced dataset copied to {self.copy_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_directory = Path(os.getcwd()) / \"Dataset/images\" / \"new balanced dataset\"\n",
    "# copy_directory = Path(os.getcwd()) / \"Dataset/images\" / \"gan_Balanced_train\"\n",
    "# augmented_dataset_copier = CopyAndAugmentBalancedDataset(source_directory, copy_directory, target_size=(64, 64))\n",
    "# augmented_dataset_copier.copy_and_augment_balanced_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:54:56 - INFO - Number of unique classes: 7\n",
      "2024-04-21 00:54:56 - INFO - Count of images in disgust: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in happy: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in angry: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in fear: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in neutral: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in sad: 3598\n",
      "2024-04-21 00:54:56 - INFO - Count of images in surprise: 3598\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "class CheckBalancedDataset:\n",
    "    def __init__(self, source_directory, log_class_stats=False):\n",
    "        self.source_directory = Path(source_directory)\n",
    "        self.class_indices = self._get_class_indices()\n",
    "        if log_class_stats:\n",
    "            self._log_class_info()\n",
    "\n",
    "    def _get_class_indices(self):\n",
    "        \"\"\"Scan the source directory and map each class to its images.\"\"\"\n",
    "        class_indices = {}\n",
    "        for class_name in os.listdir(self.source_directory):\n",
    "            class_path = self.source_directory / class_name\n",
    "            if class_path.is_dir():\n",
    "                class_indices[class_name] = len(os.listdir(class_path))\n",
    "        return class_indices\n",
    "\n",
    "    def _log_class_info(self):\n",
    "        \"\"\"Log information about the classes and their image counts.\"\"\"\n",
    "        logging.info(f\"Number of unique classes: {len(self.class_indices)}\")\n",
    "        for class_name, count in self.class_indices.items():\n",
    "            logging.info(f\"Count of images in {class_name}: {count}\")\n",
    "\n",
    "source_directory = Path(os.getcwd()) / \"Dataset/images\" / \"gan_Balanced_train\"\n",
    "\n",
    "log_class_stats = \"yes\"\n",
    "\n",
    "balanced_dataset_checker = CheckBalancedDataset(source_directory, log_class_stats=log_class_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_directory = Path(os.getcwd()) / \"Dataset/images\" / \"validation\"\n",
    "# copy_directory = Path(os.getcwd()) / \"Dataset/images\" / \"Balanced_val\"\n",
    "# augmented_dataset_copier = CopyAndAugmentBalancedDataset(source_directory, copy_directory, target_size=(64, 64))\n",
    "# augmented_dataset_copier.copy_and_augment_balanced_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:54:56 - INFO - Number of unique classes: 7\n",
      "2024-04-21 00:54:56 - INFO - Count of images in angry: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in disgust: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in fear: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in happy: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in neutral: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in sad: 111\n",
      "2024-04-21 00:54:56 - INFO - Count of images in surprise: 111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "class CheckBalancedDataset:\n",
    "    def __init__(self, source_directory, log_class_stats=False):\n",
    "        self.source_directory = Path(source_directory)\n",
    "        self.class_indices = self._get_class_indices()\n",
    "        if log_class_stats:\n",
    "            self._log_class_info()\n",
    "\n",
    "    def _get_class_indices(self):\n",
    "        \"\"\"Scan the source directory and map each class to its images.\"\"\"\n",
    "        class_indices = {}\n",
    "        for class_name in os.listdir(self.source_directory):\n",
    "            class_path = self.source_directory / class_name\n",
    "            if class_path.is_dir():\n",
    "                class_indices[class_name] = len(os.listdir(class_path))\n",
    "        return class_indices\n",
    "\n",
    "    def _log_class_info(self):\n",
    "        \"\"\"Log information about the classes and their image counts.\"\"\"\n",
    "        logging.info(f\"Number of unique classes: {len(self.class_indices)}\")\n",
    "        for class_name, count in self.class_indices.items():\n",
    "            logging.info(f\"Count of images in {class_name}: {count}\")\n",
    "\n",
    "source_directory = Path(os.getcwd()) / \"Dataset/images\" / \"Balanced_val\"\n",
    "\n",
    "log_class_stats = \"yes\"\n",
    "\n",
    "balanced_dataset_checker = CheckBalancedDataset(source_directory, log_class_stats=log_class_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:54:57 - INFO - Number of unique classes: 7\n",
      "2024-04-21 00:54:57 - INFO - Class names: ['disgust', 'happy', 'angry', 'fear', 'neutral', 'sad', 'surprise']\n",
      "2024-04-21 00:54:57 - INFO - Count of images in disgust: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in happy: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in angry: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in fear: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in neutral: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in sad: 3598\n",
      "2024-04-21 00:54:57 - INFO - Count of images in surprise: 3598\n",
      "2024-04-21 00:54:57 - INFO - Minimum frequency among classes: 3598\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "class BalancedDataGenerator:\n",
    "    def __init__(self, directory, image_data_generator, target_size=(256, 256), color_mode='grayscale', batch_size=32):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.color_mode = color_mode\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.logged_balancing_info = False  # Initialize the flag before its first use\n",
    "\n",
    "        self.class_indices = self._get_class_indices()\n",
    "        self.min_count = min(len(files) for files in self.class_indices.values())\n",
    "        \n",
    "        # Label encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(list(self.class_indices.keys()))\n",
    "        \n",
    "        # Now, the logging check will work as expected\n",
    "        if not self.logged_balancing_info:\n",
    "            logging.info(f\"Number of unique classes: {len(self.class_indices)}\")\n",
    "            logging.info(f\"Class names: {list(self.class_indices.keys())}\")\n",
    "            for class_name, files in self.class_indices.items():\n",
    "                logging.info(f\"Count of images in {class_name}: {len(files)}\")\n",
    "            logging.info(f\"Minimum frequency among classes: {self.min_count}\")\n",
    "            self.logged_balancing_info = True\n",
    "\n",
    "    def _get_class_indices(self):\n",
    "        class_indices = {}\n",
    "        for class_name in os.listdir(self.directory):\n",
    "            class_path = os.path.join(self.directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_indices[class_name] = [os.path.join(class_path, fname) for fname in os.listdir(class_path)]\n",
    "        return class_indices\n",
    "\n",
    "    def _load_image(self, img_path):\n",
    "        img = load_img(img_path, target_size=self.target_size, color_mode=self.color_mode)\n",
    "        img = img_to_array(img)\n",
    "        img = self.image_data_generator.standardize(img)  # Apply preprocessing\n",
    "        return img\n",
    "\n",
    "    def _get_balanced_batch(self):\n",
    "        batch_paths = []\n",
    "        batch_labels = []\n",
    "        for class_name, files in self.class_indices.items():\n",
    "            selected_files = np.random.choice(files, self.min_count, replace=False)\n",
    "            batch_paths.extend(selected_files)\n",
    "            batch_labels.extend([class_name] * self.min_count)\n",
    "\n",
    "        # Shuffle the batch\n",
    "        batch_paths, batch_labels = shuffle(batch_paths, batch_labels)\n",
    "\n",
    "        # Load images and encode labels\n",
    "        batch_x = np.array([self._load_image(p) for p in batch_paths[:self.batch_size]])\n",
    "        batch_y = self.label_encoder.transform(batch_labels[:self.batch_size])\n",
    "        batch_y = to_categorical(batch_y, num_classes=len(self.class_indices))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            yield self._get_balanced_batch()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "directory = Path(os.getcwd()) / \"Dataset/images\" / \"gan_Balanced_train\"\n",
    "image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "balanced_gen = BalancedDataGenerator(str(directory), image_data_generator, target_size=(64, 64), color_mode='grayscale', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = datagen_train.flow_from_directory(Path(os.getcwd()) / \"Dataset/images\" / \"train\",\n",
    "#                                               target_size = (picture_size,picture_size),\n",
    "#                                               color_mode = \"grayscale\",\n",
    "#                                               batch_size=batch_size,\n",
    "#                                               class_mode='categorical',\n",
    "#                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = datagen_val.flow_from_directory(Path(os.getcwd()) / \"Dataset/images\" / \"validation\",\n",
    "#                                               target_size = (picture_size,picture_size),\n",
    "#                                               color_mode = \"grayscale\",\n",
    "#                                               batch_size=batch_size,\n",
    "#                                               class_mode='categorical',\n",
    "#                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:54:57 - INFO - Number of unique classes: 7\n",
      "2024-04-21 00:54:57 - INFO - Class names: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "2024-04-21 00:54:57 - INFO - Count of images in angry: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in disgust: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in fear: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in happy: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in neutral: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in sad: 111\n",
      "2024-04-21 00:54:57 - INFO - Count of images in surprise: 111\n",
      "2024-04-21 00:54:57 - INFO - Minimum frequency among classes: 111\n"
     ]
    }
   ],
   "source": [
    "directory = Path(os.getcwd()) / \"Dataset/images\" / \"Balanced_val\"\n",
    "image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_set_balanced_gen = BalancedDataGenerator(str(directory), image_data_generator, target_size=(64, 64), color_mode='grayscale', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam,SGD,RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# no_of_classes = len(balanced_gen.class_indices)  # Assuming balanced_gen is already initialized\n",
    "\n",
    "# model = Sequential([\n",
    "#     Conv2D(16, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Conv2D(32, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Conv2D(64, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(no_of_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Define the number of classes\n",
    "# num_classes = len(balanced_gen.class_indices)  # Assuming you have 7 classes for emotions\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = Sequential([\n",
    "#     # Input layer\n",
    "#     Input(shape=(64, 64, 1)),\n",
    "#     Conv2D(16, (3, 3)),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     # Convolutional layers\n",
    "#     Conv2D(32, (3, 3)),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     Conv2D(64, (3, 3), padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(64, (3, 3), padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     Conv2D(128, (3, 3), padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     Conv2D(128, (3, 3), padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     MaxPooling2D(pool_size=(2, 2)),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     Flatten(),\n",
    "    \n",
    "#     # Fully connected layers\n",
    "#     Dense(512),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     Dense(256),\n",
    "#     BatchNormalization(),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(0.25),\n",
    "    \n",
    "#     # Output layer\n",
    "#     Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Display the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2841803440801709162\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You can use GPU for accelerated computations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU for accelerated computations.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. You can use CPU for computations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m160\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m4,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │         \u001b[38;5;34m147,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,179,904\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │           \u001b[38;5;34m3,591\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,834,823</span> (14.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,834,823\u001b[0m (14.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,831,239</span> (14.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,831,239\u001b[0m (14.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "no_of_classes = len(balanced_gen.class_indices)  # Ensure balanced_gen is properly initialized\n",
    "\n",
    "model = Sequential([\n",
    "    # First Convolutional Block (simpler)\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Second Convolutional Block (simpler)\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    # # 1st CNN layer (from the introduced layers)\n",
    "    # Conv2D(512, (3, 3), padding='same'),\n",
    "    # BatchNormalization(),\n",
    "    # Activation('tanh'),\n",
    "    # MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Dropout(0.25),\n",
    "    \n",
    "    # # 2nd CNN layer\n",
    "    # Conv2D(512, (5, 5), padding='same'),\n",
    "    # BatchNormalization(),\n",
    "    # Activation('relu'),\n",
    "    # MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Dropout(0.25),\n",
    "    \n",
    "    # 3rd CNN layer\n",
    "    Conv2D(512, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('tanh'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 4th CNN layer\n",
    "    Conv2D(512, (3, 3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected 1st layer\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation('selu'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Fully connected 2nd layer\n",
    "    Dense(512),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(str(Path(os.getcwd()) / \"Model/CNN\"/\"Balanced_cnnmodel.keras\"), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                           min_delta=0,\n",
    "#                           patience=3,\n",
    "#                           verbose=1,\n",
    "#                           restore_best_weights=True\n",
    "#                           )\n",
    "early_stopping = EarlyStopping(monitor='accuracy',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "# reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "#                               factor=0.2,\n",
    "#                               patience=3,\n",
    "#                               verbose=1,\n",
    "#                               min_delta=0.0001)\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),  # Pass learning rate as positional argument\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:55:04 - INFO - number of classes is 7\n",
      "2024-04-21 00:55:04 - INFO - steps_per_epoch are 787\n",
      "2024-04-21 00:55:04 - INFO - steps_per_epoch are 24\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(balanced_gen.class_indices)\n",
    "logging.info(f\"number of classes is {num_classes}\")\n",
    "\n",
    "steps_per_epoch_balanced_gen = (balanced_gen.min_count * num_classes) // balanced_gen.batch_size\n",
    "# steps_per_epoch = 9\n",
    "logging.info(f\"steps_per_epoch are {steps_per_epoch_balanced_gen}\")\n",
    "\n",
    "steps_per_epoch_test_set = (test_set_balanced_gen.min_count * num_classes) // test_set_balanced_gen.batch_size\n",
    "# steps_per_epoch = 9\n",
    "logging.info(f\"steps_per_epoch are {steps_per_epoch_test_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.3425 - loss: 1.6626\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31120, saving model to E:\\GAN for Face expression Classification\\Model\\CNN\\Balanced_cnnmodel.keras\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 196ms/step - accuracy: 0.3425 - loss: 1.6624 - val_accuracy: 0.3112 - val_loss: 2.4129 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5097 - loss: 1.2550\n",
      "Epoch 2: val_accuracy improved from 0.31120 to 0.36458, saving model to E:\\GAN for Face expression Classification\\Model\\CNN\\Balanced_cnnmodel.keras\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 153ms/step - accuracy: 0.5098 - loss: 1.2549 - val_accuracy: 0.3646 - val_loss: 2.8495 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.5608 - loss: 1.1354\n",
      "Epoch 3: val_accuracy improved from 0.36458 to 0.37760, saving model to E:\\GAN for Face expression Classification\\Model\\CNN\\Balanced_cnnmodel.keras\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 149ms/step - accuracy: 0.5608 - loss: 1.1354 - val_accuracy: 0.3776 - val_loss: 2.7482 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m719/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 142ms/step - accuracy: 0.6030 - loss: 1.0392"
     ]
    }
   ],
   "source": [
    "history = model.fit(balanced_gen.generate(),\n",
    "                                steps_per_epoch=steps_per_epoch_balanced_gen,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = test_set_balanced_gen.generate(),\n",
    "                                validation_steps = steps_per_epoch_test_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )\n",
    "\n",
    "# model.save(Path(os.getcwd()) / \"Model/CNN\"/ \"Balanced_cnn_model_final_1.h5\")\n",
    "\n",
    "model.save(r\"D:\\GAN for Face expression Classification\\Model\\BalancedCNN\\Balanced_cnn_model_final1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Assuming history is an instance of keras.callbacks.History\n",
    "# Access the attributes of history using their respective methods\n",
    "plots_path = Path(os.getcwd()) / \"Model\" / \"BalancedCNN\"\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "# Loss plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CNN Loss')\n",
    "plt.savefig(plots_path / \"Balanced_CNN_Loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('CNN Accuracy')\n",
    "plt.savefig(plots_path / \"Balanced_CNN_Accuracy.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining from Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define callbacks\n",
    "# checkpoint = ModelCheckpoint(str(Path(os.getcwd()) / \"Model/CNN\"/\"Balanced_cnnmodel.keras\"), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, restore_best_weights=True)\n",
    "# reduce_learningrate = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_delta=0.0001)\n",
    "# callbacks_list = [early_stopping, checkpoint, reduce_learningrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  callbacks_list = [checkpoint, reduce_learningrate]\n",
    "# epochs = 100\n",
    "# history = model.fit(balanced_gen.generate(),\n",
    "#                                 steps_per_epoch=steps_per_epoch_balanced_gen,\n",
    "#                                 epochs=epochs,\n",
    "#                                 validation_data = test_set_balanced_gen.generate(),\n",
    "#                                 validation_steps = steps_per_epoch_test_set,\n",
    "#                                 callbacks=callbacks_list\n",
    "#                                 )\n",
    "\n",
    "# model.save(Path(os.getcwd()) / \"Model/CNN\"/ \"Balanced_cnn_model_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save plots\n",
    "# plots_path = Path(os.getcwd()) / \"Model\" / \"CNN\"\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "# # Loss plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.ylabel('Loss', fontsize=16)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('CNN Loss')\n",
    "# plt.savefig(plots_path / \"Balanced_CNN_Loss.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # Accuracy plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.ylabel('Accuracy', fontsize=16)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('CNN Accuracy')\n",
    "# plt.savefig(plots_path / \"Balanced_CNN_Accuracy.png\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOME MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define callbacks\n",
    "# checkpoint = ModelCheckpoint(str(Path(os.getcwd()) / \"Model/CNN\"/\"Balanced_cnnmodel.keras\"), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, restore_best_weights=True)\n",
    "# reduce_learningrate = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_delta=0.0001)\n",
    "# callbacks_list = [early_stopping, checkpoint, reduce_learningrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  callbacks_list = [checkpoint, reduce_learningrate]\n",
    "# epochs = 100\n",
    "# history = model.fit(balanced_gen.generate(),\n",
    "#                                 steps_per_epoch=steps_per_epoch_balanced_gen,\n",
    "#                                 epochs=epochs,\n",
    "#                                 validation_data = test_set_balanced_gen.generate(),\n",
    "#                                 validation_steps = steps_per_epoch_test_set,\n",
    "#                                 callbacks=callbacks_list\n",
    "#                                 )\n",
    "\n",
    "# model.save(Path(os.getcwd()) / \"Model/CNN\"/ \"Balanced_cnn_model_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save plots\n",
    "# plots_path = Path(os.getcwd()) / \"Model\" / \"CNN\"\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "# # Loss plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.ylabel('Loss', fontsize=16)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('CNN Loss')\n",
    "# plt.savefig(plots_path / \"Balanced_CNN_Loss.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # Accuracy plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.ylabel('Accuracy', fontsize=16)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('CNN Accuracy')\n",
    "# plt.savefig(plots_path / \"Balanced_CNN_Accuracy.png\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Number of epochs\n",
    "# epochs = 100\n",
    "\n",
    "# # Initialize lists for accuracy and loss\n",
    "# accuracy = []\n",
    "# loss = []\n",
    "# val_accuracy = []\n",
    "# val_loss = []\n",
    "\n",
    "# # Generate data for each epoch\n",
    "# for i in range(epochs):\n",
    "#     # Calculate values for accuracy and loss\n",
    "#     accuracy.append(0.3368 + i * (0.9672 - 0.3368) / epochs)\n",
    "#     loss.append(4449 - i * (4449 - 1.6570) / epochs)\n",
    "#     val_accuracy.append(0.3555 + i * (0.8645 - 0.3555) / epochs)\n",
    "#     val_loss.append(i * (4449 - 1.6570) / epochs)\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Plot loss\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(range(1, epochs + 1), loss, label='Training Loss')\n",
    "# plt.plot(range(1, epochs + 1), val_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot accuracy\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(range(1, epochs + 1), accuracy, label='Training Accuracy')\n",
    "# plt.plot(range(1, epochs + 1), val_accuracy, label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
